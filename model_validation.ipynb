{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation: The Bias-Variance Tradeoff and the Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- explain the bias-variance tradeoff and the correlative notions of underfit and overfit models;\n",
    "- describe a train-test split and explain its purpose in the context of predictive statistics / machine learning;\n",
    "- explain the algorithm of cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Tradeoff\n",
    "\n",
    "**High-bias** algorithms tend to be less complex, with simple or rigid underlying structure.\n",
    "\n",
    "+ They train models that are consistent, but inaccurate on average.\n",
    "+ These include linear or parametric algorithms such as regression and naive Bayes.\n",
    "+ The following sorts of difficulties could lead to high bias:\n",
    "      - We did not include the correct predictors;\n",
    "      - We did not take interactions into account;\n",
    "      - We missed a non-linear (polynomial) relationship. \n",
    "      \n",
    "High-bias models are generally **underfit**: The models have not picked up enough of the signal in the data. And so even though they may be consistent, they don't perform particularly well on the initial data, and so they will be consistently inaccurate.\n",
    "\n",
    "On the other hand, **high-variance** algorithms tend to be more complex, with flexible underlying structure.\n",
    "\n",
    "+ They train models that are accurate on average, but inconsistent.\n",
    "+ These include non-linear or non-parametric algorithms such as decision trees and nearest-neighbor models.\n",
    "+ The following sorts of difficulties could lead to high variance:\n",
    "      - We included an unreasonably large number of predictors;\n",
    "      - We created new features by squaring and cubing each feature.\n",
    "\n",
    "High variance models are **overfit**: The models have picked up on the noise as well as the signal in the data. And so even though they may perform well on the initial data, they will be inconsistently accurate on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we build our models, we have to keep this relationship in mind.  If we build complex models, we risk overfitting our models.  Their predictions will vary greatly when introduced to new data.  If our models are too simple, the predictions as a whole will be inaccurate.   \n",
    "\n",
    "The goal is to build a model with enough complexity to be accurate, but not too much complexity to be erratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimal](img/optimal_bias_variance.png)\n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n",
    "### Let's take a look at our familiar King County housing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "df = pd.read_csv('data/king_county.csv', index_col='id')\n",
    "df = df.iloc[:, :12]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Let's generate random subsets of our data\n",
    "\n",
    "# Date is not in the correct format so we are dropping it for now.\n",
    "\n",
    "sample_point = df.drop('price', axis=1).sample(1)\n",
    "point_preds = []\n",
    "\n",
    "r_2 = []\n",
    "simple_rmse = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    df_sample = df.sample(5000, replace=True)\n",
    "    y = df_sample.price\n",
    "    X = df_sample.drop('price', axis=1)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    \n",
    "    y_hat = lr.predict(X)\n",
    "    simple_rmse.append(np.sqrt(mean_squared_error(y, y_hat)))\n",
    "    r_2.append(lr.score(X, y))\n",
    "    \n",
    "    y_hat_point = lr.predict(sample_point)\n",
    "    \n",
    "    point_preds.append(y_hat_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'simple mean: {np.mean(simple_rmse)}')\n",
    "print(f'simple variance: {np.var(point_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/king_county.csv', index_col='id')\n",
    "\n",
    "pf = PolynomialFeatures(2)\n",
    "\n",
    "df_poly = pd.DataFrame(pf.fit_transform(df.drop('price', axis=1)))\n",
    "df_poly.index = df.index\n",
    "df_poly['price'] = df['price']\n",
    "\n",
    "cols = list(df_poly)\n",
    "\n",
    "# move the column to head of list using index, pop and insert\n",
    "cols.insert(0, cols.pop(cols.index('price')))\n",
    "\n",
    "df_poly = df_poly.loc[:, cols]\n",
    "\n",
    "df_poly.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "sample_point = df_poly.drop('price', axis=1).sample(1)\n",
    "\n",
    "r_2 = []\n",
    "point_preds_comp = []\n",
    "complex_rmse = []\n",
    "for i in range(100):\n",
    "    \n",
    "    df_sample = df_poly.sample(1000, replace=True)\n",
    "    y = df_sample.price\n",
    "    X = df_sample.drop('price', axis=1)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    y_hat = lr.predict(X)\n",
    "    complex_rmse.append(np.sqrt(mean_squared_error(y, y_hat)))\n",
    "    r_2.append(lr.score(X, y))\n",
    "    \n",
    "    y_hat_point = lr.predict(sample_point)\n",
    "    \n",
    "    point_preds_comp.append(y_hat_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'simple mean {np.mean(simple_rmse)}')\n",
    "print(f'complex mean {np.mean(complex_rmse)}')\n",
    "\n",
    "print(f'simple variance {np.var(point_preds)}')\n",
    "print(f'complex variance {np.var(point_preds_comp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![which_model](img/which_model_is_better_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "It is hard to know if your model is too simple or complex by just using it on training data.\n",
    "\n",
    "We can hold out part of our training sample, and use it as a test sample and use it to monitor our prediction error.\n",
    "\n",
    "This allows us to evaluate whether our model has the right balance of bias/variance. \n",
    "\n",
    "<img src='img/testtrainsplit.png' width =550 />\n",
    "\n",
    "* **training set** —a subset to train a model.\n",
    "* **test set**—a subset to test the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/king_county.csv', index_col='id')\n",
    "\n",
    "y = df.price\n",
    "X = df[['bedrooms', 'sqft_living']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=None,\n",
    "                                                   random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(X_train.shape[0] == y_train.shape[0])\n",
    "print(X_test.shape[0] == y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we know if our model is overfitting or underfitting?\n",
    "\n",
    "If our model is not performing well on the training  data, we are probably underfitting it.  \n",
    "\n",
    "\n",
    "To know if our  model is overfitting the data, we need  to test our model on unseen data. \n",
    "We then measure our performance on the unseen data. \n",
    "\n",
    "If the model performs significantly worse on the  unseen data, it is probably  overfitting the data.\n",
    "\n",
    "<img src='https://developers.google.com/machine-learning/crash-course/images/WorkflowWithTestSet.svg' width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Consider the following scenarios and describe them according to bias and variance. There are four possibilities:\n",
    "\n",
    "- a. The model has low bias and high variance.\n",
    "- b. The model has high bias and low variance.\n",
    "- c. The model has both low bias and low variance.\n",
    "- d. The model has both high bias and high variance.\n",
    "\n",
    "**Scenario 1**: The model has a low RMSE on training and a low RMSE on test.\n",
    "<details>\n",
    "    <summary> Anwer\n",
    "    </summary>\n",
    "    c. The model has both low bias and low variance.\n",
    "    </details>\n",
    "\n",
    "**Scenario 2**: The model has a high $R^2$ on the training set, but a low $R^2$ on the test.\n",
    "<details>\n",
    "    <summary> Anwer\n",
    "    </summary>\n",
    "    a. The model has low bias and high variance.\n",
    "    </details>\n",
    "\n",
    "**Scenario 3**: The model performs well on data it is fit on and well on data it has not seen.\n",
    "<details>\n",
    "    <summary> Anwer\n",
    "    </summary>\n",
    "    c. The model has both low bias and low variance.\n",
    "    </details>\n",
    "    \n",
    "**Scenario 4**: The model has a low $R^2$ on training but high on the test set.\n",
    "<details>\n",
    "    <summary> Anwer\n",
    "    </summary>\n",
    "    d. The model has both high bias and high variance.\n",
    "    </details>\n",
    "    \n",
    "**Scenario 5**: The model leaves out many of the meaningful predictors, but is consistent across samples.\n",
    "<details>\n",
    "    <summary> Anwer\n",
    "    </summary>\n",
    "    b. The model has high bias and low variance.\n",
    "    </details>\n",
    "    \n",
    "**Scenario 6**: The model is highly sensitive to random noise in the training set.\n",
    "<details>\n",
    "    <summary> Anwer\n",
    "    </summary>\n",
    "    a. The model has low bias and high variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should you ever fit on your test set?  \n",
    "\n",
    "\n",
    "![no](https://media.giphy.com/media/d10dMmzqCYqQ0/giphy.gif)\n",
    "\n",
    "\n",
    "**Never fit on test data.** If you are seeing surprisingly good results on your evaluation metrics, it might be a sign that you are accidentally training on the test set.\n",
    "\n",
    "Let's go back to our KC housing data without the polynomial transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/king_county.csv', index_col='id')\n",
    "\n",
    "# Date  is not in the correct format so we are dropping it for now.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a train-test split via the `sklearn.model_selection` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "y = df.price\n",
    "X = df[['bedrooms', 'sqft_living']]\n",
    "\n",
    "# Here is the convention for a traditional train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanstiate your linear regression object\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the training set\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the R^2 of the training data\n",
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A .506 R-squared reflects a model that explains about half of the total variance in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check performance on test data\n",
    "\n",
    "Next, we test how well the model performs on the unseen test data. Remember, we do not fit the model again. The model has calculated the optimal parameters learning from the training set.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge check\n",
    "How would you describe the bias of the model based on the above training $R^2$?\n",
    "\n",
    "The difference between the train and test scores is low.\n",
    "\n",
    "What does that indicate about variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same procedure with a polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/king_county.csv', index_col='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_2 = PolynomialFeatures(4)\n",
    "\n",
    "X_poly = pd.DataFrame(\n",
    "            poly_2.fit_transform(df.drop('price', axis=1))\n",
    "                      )\n",
    "\n",
    "y = df.price\n",
    "X_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "lr_poly = LinearRegression()\n",
    "\n",
    "# Always fit on the training set\n",
    "lr_poly.fit(X_train, y_train)\n",
    "\n",
    "lr_poly.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_poly.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "[This post about scaling and data leakage](https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data) explains that if you are going to scale your data, you should only train your scaler on the training data to prevent data leakage.  \n",
    "\n",
    "Perform the same train-test split as shown above for the simple model, but now scale your data appropriately.  \n",
    "\n",
    "The $R^2$ for both train and test should be the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "y = df.price\n",
    "X = df[['bedrooms', 'sqft_living']]\n",
    "\n",
    "# Train test split with random_state=42 and test_size=0.2\n",
    "\n",
    "# Scale appropriately\n",
    "\n",
    "# fit and score the model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kfolds: Even More Rigorous Validation  \n",
    "\n",
    "For a more rigorous cross-validation, we turn to K-folds\n",
    "\n",
    "![kfolds](img/k_folds.png)\n",
    "\n",
    "[image via sklearn](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this process, we split the dataset into train and test as usual, then we perform a shuffling train-test split on the training set.  \n",
    "\n",
    "KFolds holds out one fraction of the dataset, trains on the larger fraction, then calculates a test score on the held out set. It repeats this process until each group has served as the test set.\n",
    "\n",
    "We tune our parameters on the training set using k-many folds, then validate on the test data. This allows us to build our model and check to see if it is overfit without touching the test data set. This protects our model from bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('price', axis=1)\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "for train_ind, test_ind in kf.split(X, y):\n",
    "    \n",
    "    X_train, y_train = X.iloc[train_ind], y.iloc[train_ind]\n",
    "    X_test, y_test = X.iloc[test_ind], y.iloc[test_ind]\n",
    "    \n",
    "    lr.fit(X_train, y_train)\n",
    "    train_r2.append(lr.score(X_train, y_train))\n",
    "    test_r2.append(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean train r_2\n",
    "np.mean(train_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean test r_2\n",
    "np.mean(test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out our polynomial model\n",
    "poly_2 = PolynomialFeatures(2)\n",
    "\n",
    "df_poly = pd.DataFrame(\n",
    "            poly_2.fit_transform(df.drop('price', axis=1))\n",
    "                      )\n",
    "\n",
    "X = df_poly\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "for train_ind, test_ind in kf.split(X, y):\n",
    "    \n",
    "    X_train, y_train = X.iloc[train_ind], y.iloc[train_ind]\n",
    "    X_test, y_test = X.iloc[test_ind], y.iloc[test_ind]\n",
    "    \n",
    "    lr.fit(X_train, y_train)\n",
    "    train_r2.append(lr.score(X_train, y_train))\n",
    "    test_r2.append(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean train r_2\n",
    "np.mean(train_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean test r_2\n",
    "np.mean(test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an acceptable model, we train our model on the entire training set, and score on the test to validate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
